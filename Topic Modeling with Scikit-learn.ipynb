{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "169d8676",
   "metadata": {
    "id": "169d8676"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import string\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "kGvTIo-PkUYG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "kGvTIo-PkUYG",
    "outputId": "948f0b10-9b17-473e-d0c1-f4546823a6bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>all_text</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>hour</th>\n",
       "      <th>Class</th>\n",
       "      <th>body_len</th>\n",
       "      <th>text_no_emojis</th>\n",
       "      <th>language</th>\n",
       "      <th>cleaned_tokenized_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>x2wyl4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>domesticviolence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I can't even eat chocolate without thinking ab...</td>\n",
       "      <td>2022-09-01 10:22:54</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "      <td>I can't even eat chocolate without thinking ab...</td>\n",
       "      <td>en</td>\n",
       "      <td>['eat', 'chocolate', 'thinking', 'orderedeaten...</td>\n",
       "      <td>['eat', 'chocolate', 'think', 'orderedeaten', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>x2uv4c</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>domesticviolence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>How do I know he's really changing? I think th...</td>\n",
       "      <td>2022-09-01 08:42:03</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>380</td>\n",
       "      <td>How do I know he's really changing? I think th...</td>\n",
       "      <td>en</td>\n",
       "      <td>['changing', 'think', 'time', 'changing', 'tim...</td>\n",
       "      <td>['change', 'think', 'time', 'change', 'time', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>x2pn4w</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>domesticviolence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Just got my 5th NFA ðŸ˜­ This is more of a rant t...</td>\n",
       "      <td>2022-09-01 04:46:37</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>732</td>\n",
       "      <td>Just got my 5th NFA :loudly_crying_face: This ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['nfa', 'loudly_crying_face', 'rant', 'broken'...</td>\n",
       "      <td>['nfa', 'loudly_crying_face', 'rant', 'broken'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id  num_comments  score         subreddit  upvote_ratio  \\\n",
       "0           0  x2wyl4           1.0    1.0  domesticviolence           1.0   \n",
       "1           1  x2uv4c           1.0    1.0  domesticviolence           1.0   \n",
       "2           2  x2pn4w           1.0    1.0  domesticviolence           1.0   \n",
       "\n",
       "                                            all_text             Datetime  \\\n",
       "0  I can't even eat chocolate without thinking ab...  2022-09-01 10:22:54   \n",
       "1  How do I know he's really changing? I think th...  2022-09-01 08:42:03   \n",
       "2  Just got my 5th NFA ðŸ˜­ This is more of a rant t...  2022-09-01 04:46:37   \n",
       "\n",
       "   hour  Class  body_len                                     text_no_emojis  \\\n",
       "0    10      1       356  I can't even eat chocolate without thinking ab...   \n",
       "1     8      1       380  How do I know he's really changing? I think th...   \n",
       "2     4      1       732  Just got my 5th NFA :loudly_crying_face: This ...   \n",
       "\n",
       "  language                             cleaned_tokenized_text  \\\n",
       "0       en  ['eat', 'chocolate', 'thinking', 'orderedeaten...   \n",
       "1       en  ['changing', 'think', 'time', 'changing', 'tim...   \n",
       "2       en  ['nfa', 'loudly_crying_face', 'rant', 'broken'...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  ['eat', 'chocolate', 'think', 'orderedeaten', ...  \n",
       "1  ['change', 'think', 'time', 'change', 'time', ...  \n",
       "2  ['nfa', 'loudly_crying_face', 'rant', 'broken'...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processed.csv',index_col=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zHY6TM_cJq3g",
   "metadata": {
    "id": "zHY6TM_cJq3g"
   },
   "source": [
    "Compare and test LDA and NMF performance on domesticviolence subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930821bb",
   "metadata": {
    "id": "930821bb"
   },
   "outputs": [],
   "source": [
    "dv= df.loc[df['subreddit'] == 'domesticviolence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc782e",
   "metadata": {
    "id": "59bc782e"
   },
   "outputs": [],
   "source": [
    "#starting to experiment with CountVectorizer(max_df=0.8, min_df=5, ngram_range=(1,2))\n",
    "count_vect_0 = CountVectorizer(max_df=0.8, min_df=5, ngram_range=(1,2), stop_words='english')\n",
    "dv_doc_term_matrix_0 = count_vect_0.fit_transform(dv['lemmatized_text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75acbf0",
   "metadata": {
    "collapsed": true,
    "id": "f75acbf0"
   },
   "outputs": [],
   "source": [
    "LDA_3_topics= LatentDirichletAllocation(n_components=3,max_iter=50)\n",
    "dv_LDA_0 = LDA_3_topics.fit(dv_doc_term_matrix_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d279f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "418d279f",
    "outputId": "67296b65-d93d-4388-ddb4-c895990002dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['aware past', 'adult daughter', 'agree live', 'american way', 'attractive', 'baby wongs', 'block unblocked', 'bother write', 'abhors', 'anytime thing']\n",
      "Top 10 words for topic #1:\n",
      "['agh', 'attempt head', 'arm lay', 'bf country', 'advice friendship', 'behave inappropriately', 'bit fall', 'ask ear', 'block unblocked', 'big struggle']\n",
      "Top 10 words for topic #2:\n",
      "['approve try', 'abhors', 'asks rabbit', 'bother write', 'bar misc', 'bf country', 'big struggle', 'block unblocked', 'ask ear', 'amp sort']\n"
     ]
    }
   ],
   "source": [
    "#Print the 10 words with highest probabilities for all the topics\n",
    "for i,topic in enumerate(dv_LDA_0.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect_0.get_feature_names_out()[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mz8acREXq9M4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mz8acREXq9M4",
    "outputId": "d595f69d-a8ea-4173-bb0b-9fa74b7b08b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for topic #0:\n",
      "['agree man', 'approve try', 'alloy', 'able relate', 'bit fall', 'boring ask', 'arm lay', 'anime character', 'big struggle', 'bless rain', 'aware past', 'adult daughter', 'agree live', 'american way', 'attractive', 'baby wongs', 'block unblocked', 'bother write', 'abhors', 'anytime thing']\n",
      "Top 20 words for topic #1:\n",
      "['australia btw', 'best employee', 'account cause', 'age decide', 'boring ask', 'apartment people', 'apologize stop', 'bother write', 'bastard child', 'apart thats', 'agh', 'attempt head', 'arm lay', 'bf country', 'advice friendship', 'behave inappropriately', 'bit fall', 'ask ear', 'block unblocked', 'big struggle']\n",
      "Top 20 words for topic #2:\n",
      "['agh', 'best employee', 'away reason', 'body healthy', 'able relate', 'anime character', 'ampxb believe', 'ask big', 'bit fall', 'bf shower', 'approve try', 'abhors', 'asks rabbit', 'bother write', 'bar misc', 'bf country', 'big struggle', 'block unblocked', 'ask ear', 'amp sort']\n"
     ]
    }
   ],
   "source": [
    "#Print the 20 words with highest probabilities for all the topics for dv\n",
    "for i,topic in enumerate(dv_LDA_0.components_):\n",
    "    print(f'Top 20 words for topic #{i}:')\n",
    "    print([count_vect_0.get_feature_names_out()[i] for i in topic.argsort()[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3SOW1aDnq8_L",
   "metadata": {
    "id": "3SOW1aDnq8_L"
   },
   "outputs": [],
   "source": [
    "# I don't see any keyowrds indicating violence and abuse, and those two grams don't make much sense, try unigram vectorizer\n",
    "#experiemnt with CountVectorizer(max_df=0.95, min_df=2, ngram_range=(1,1))\n",
    "count_vect_1 = CountVectorizer(max_df=0.9, min_df=3, ngram_range=(1,1), stop_words='english')\n",
    "dv_doc_term_matrix_1 = count_vect_1.fit_transform(dv['lemmatized_text'].values.astype('U'))\n",
    "non_dv_doc_term_matrix_1 = count_vect_1.fit_transform(non_dv['lemmatized_text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OPrzP7D4pCW6",
   "metadata": {
    "id": "OPrzP7D4pCW6"
   },
   "outputs": [],
   "source": [
    "LDA_3_topics = LatentDirichletAllocation(n_components=3, max_iter=50)\n",
    "dv_LDA_1 = LDA_3_topics.fit(dv_doc_term_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-AdbeuhgpK2I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-AdbeuhgpK2I",
    "outputId": "a967f13d-809f-4522-e8e4-2e149fec8fdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['cavy', 'calcium', 'aaand', 'cozied', 'eap', 'directionless', 'disciplinary', 'drift', 'cash', 'bereft']\n",
      "Top 10 words for topic #1:\n",
      "['backpacking', 'cocktail', 'cisco', 'anon', 'bellow', 'drift', 'confection', 'eap', 'aaand', 'boyfriendgirlfriend']\n",
      "Top 10 words for topic #2:\n",
      "['assistant', 'brand', 'carlsberg', 'directionless', 'aot', 'disrupt', 'demonstrative', 'drift', 'cash', 'disciplinary']\n"
     ]
    }
   ],
   "source": [
    "#Print the 10 words with highest probabilities for all the topics for dv\n",
    "for i,topic in enumerate(dv_LDA_1.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect_1.get_feature_names_out()[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd24bdc",
   "metadata": {
    "id": "1dd24bdc"
   },
   "outputs": [],
   "source": [
    "LDA_5_topics = LatentDirichletAllocation(n_components=5, max_iter=50)\n",
    "dv_LDA_2 = LDA_5_topics.fit(dv_doc_term_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194518d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "194518d8",
    "outputId": "cf22ec6d-402e-480b-d6f5-7c584b926394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for topic #0:\n",
      "['berate', 'conducive', 'artillery', 'ebbed', 'deja', 'elate', 'clunky', 'anniversary', 'battery', 'cryptid', 'dylan', 'abandonment', 'antigay', 'assthats', 'abandon', 'commenter', 'eachother', 'consume', 'barberqueue', 'britain']\n",
      "Top 20 words for topic #1:\n",
      "['amplifier', 'depressant', 'clunky', 'bruh', 'augustseptember', 'abandon', 'buff', 'celiac', 'biomedical', 'degrades', 'ebbed', 'comprehend', 'catalog', 'anxietyinducing', 'clergyman', 'auction', 'elm', 'britain', 'bindr', 'clawed']\n",
      "Top 20 words for topic #2:\n",
      "['distinctive', 'accompany', 'abba', 'actualy', 'dopey', 'celiac', 'abandon', 'cross', 'checker', 'cellar', 'aum', 'cervical', 'cleanup', 'caretaking', 'cgpa', 'berate', 'djok', 'ebbed', 'elm', 'bismol']\n",
      "Top 20 words for topic #3:\n",
      "['confucius', 'angular', 'cypriot', 'buff', 'bruh', 'biggie', 'bard', 'advicesupport', 'distinctive', 'darken', 'celiac', 'aum', 'cockney', 'brotherm', 'ebbed', 'cgpa', 'dopey', 'apron', 'devestated', 'djok']\n",
      "Top 20 words for topic #4:\n",
      "['confides', 'darken', 'abba', 'devestated', 'elm', 'conducive', 'ecret', 'caretaking', 'dishonesty', 'bobcat', 'checker', 'dopey', 'abandon', 'distinctly', 'cross', 'djok', 'ebbed', 'distinctive', 'bismol', 'cgpa']\n"
     ]
    }
   ],
   "source": [
    "#Print the 20 words with highest probabilities for all the five topics \n",
    "for i,topic in enumerate(dv_LDA_2.components_):\n",
    "    print(f'Top 20 words for topic #{i}:')\n",
    "    print([count_vect_1.get_feature_names_out()[i] for i in topic.argsort()[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d1f50",
   "metadata": {
    "id": "a50d1f50"
   },
   "outputs": [],
   "source": [
    "LDA_10_topics = LatentDirichletAllocation(n_components=10, max_iter=50)\n",
    "dv_LDA_3 = LDA_10_topics.fit(dv_doc_term_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3A5FzxEt6Re4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3A5FzxEt6Re4",
    "outputId": "4f88326e-4716-44c4-b4a9-3dc42821b1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for topic #0:\n",
      "['cleanup', 'adversity', 'correspond', 'abandon', 'adviser', 'ccepted', 'djok', 'black_mediumsmall_squareim', 'anniversary', 'ebbed', 'abandonment', 'cupid', 'elm', 'cryptid', 'artillery', 'antigay', 'consume', 'berate', 'assthats', 'commenter']\n",
      "Top 20 words for topic #1:\n",
      "['breaker', 'cleanup', 'berate', 'devestated', 'ecret', 'dishonesty', 'abba', 'checker', 'abandon', 'dopey', 'bobcat', 'distinctly', 'caretaking', 'elm', 'cross', 'distinctive', 'djok', 'ebbed', 'cgpa', 'bismol']\n",
      "Top 20 words for topic #2:\n",
      "['bothersome', 'albino', 'apron', 'chaste', 'centre', 'bshells', 'dickens', 'abandon', 'aum', 'ecret', 'boeing', 'distracts', 'cgpa', 'elate', 'emanate', 'acute', 'conducive', 'checker', 'cervical', 'actualy']\n",
      "Top 20 words for topic #3:\n",
      "['barbarian', 'djok', 'buff', 'cgpa', 'depressant', 'abandon', 'britain', 'augustseptember', 'ebbed', 'amplifier', 'anxietyinducing', 'catalog', 'comprehend', 'biomedical', 'elm', 'bindr', 'degrades', 'clergyman', 'auction', 'clawed']\n",
      "Top 20 words for topic #4:\n",
      "['counciling', 'cypriot', 'dicey', 'cockney', 'aum', 'breaker', 'dishonesty', 'elon', 'biggie', 'advicesupport', 'celiac', 'darken', 'distinctive', 'apron', 'brotherm', 'ebbed', 'dopey', 'cgpa', 'devestated', 'djok']\n",
      "Top 20 words for topic #5:\n",
      "['bud', 'complain', 'elearning', 'chronologically', 'cleanup', 'bruh', 'bismol', 'edgewise', 'bass', 'ebbed', 'ani', 'conversate', 'deliver', 'cervical', 'agender', 'checker', 'cockney', 'djok', 'elm', 'aum']\n",
      "Top 20 words for topic #6:\n",
      "['aum', 'dopey', 'consensual', 'caretaking', 'elm', 'concise', 'cleanup', 'changer', 'angular', 'bobcat', 'djok', 'ebbed', 'bruh', 'buff', 'britain', 'clunky', 'carpet', 'cleanliness', 'celiac', 'elearning']\n",
      "Top 20 words for topic #7:\n",
      "['bobcat', 'abba', 'bulletin', 'bindr', 'computation', 'beyonce', 'academia', 'elate', 'anxietyinducing', 'battery', 'conducive', 'disallowed', 'clunky', 'dylan', 'abandonment', 'deja', 'barberqueue', 'eachother', 'abandon', 'britain']\n",
      "Top 20 words for topic #8:\n",
      "['bizarre', 'blatant', 'considerably', 'crevice', 'carreer', 'antiseptic', 'annoy', 'bondage', 'cozy', 'bore', 'chronic', 'brass', 'dysfunction', 'cp', 'abandon', 'chewie', 'decidedly', 'elate', 'boredom', 'deceptive']\n",
      "Top 20 words for topic #9:\n",
      "['darth', 'antigay', 'brandon', 'confucius', 'bruh', 'brie', 'cockney', 'carlin', 'admin', 'district', 'cnn', 'djok', 'britain', 'adversity', 'catasstrophy', 'bard', 'buff', 'apron', 'aspect', 'consume']\n"
     ]
    }
   ],
   "source": [
    "#Print the 20 words with highest probabilities for all the five topics \n",
    "for i,topic in enumerate(dv_LDA_3.components_):\n",
    "    print(f'Top 20 words for topic #{i}:')\n",
    "    print([count_vect_1.get_feature_names_out()[i] for i in topic.argsort()[-20:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EU4N2zI_FeKD",
   "metadata": {
    "id": "EU4N2zI_FeKD"
   },
   "source": [
    "all the models so far are not satisfactory, because they don't converge on topics that indicate violence. let's try NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FpPRknfrGRlx",
   "metadata": {
    "id": "FpPRknfrGRlx"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "dv_tfidf_0 = tfidf_vectorizer.fit_transform(df['lemmatized_text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a3729",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "c53a3729",
    "outputId": "a7dad64c-cb0f-497f-dd1a-72d4d2d0aa09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dv_nmf_0 = NMF(n_components=3, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(dv_tfidf_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-ewd1zgyFnkc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ewd1zgyFnkc",
    "outputId": "23ceac34-70c4-4c84-f711-f834f43c9fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for topic #0:\n",
      "['talk', 'come', 'need', 'love', 'day', 'think', 'relationship', 'help', 'start', 'ive', 'abuse', 'friend', 'leave', 'try', 'year', 'thing', 'time', 'feel', 'want', 'like']\n",
      "Top 20 words for topic #1:\n",
      "['school', 'old', 'sibling', 'kid', 'abuse', 'doesnt', 'hit', 'child', 'home', 'live', 'house', 'help', 'family', 'parent', 'father', 'mother', 'brother', 'sister', 'dad', 'mom']\n",
      "Top 20 words for topic #2:\n",
      "['long', 'jail', 'relationship', 'away', 'need', 'story', 'dv', 'change', 'ex', 'post', 'boyfriend', 'abusive', 'victim', 'advice', 'abuse', 'violence', 'abuser', 'domestic', 'help', 'delete']\n"
     ]
    }
   ],
   "source": [
    "#Print the 20 words with highest probabilities for all the topics \n",
    "for i,topic in enumerate(dv_nmf_0.components_):\n",
    "    print(f'Top 20 words for topic #{i}:')\n",
    "    print([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-20:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pONUn2RmHrGB",
   "metadata": {
    "id": "pONUn2RmHrGB"
   },
   "source": [
    "NMF is so much faster and the results are so much better! the 3rd topic is clearly about abuse and domestic violence. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c668505f",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "1. Gensim gave better results, it captured domestic violence topics every time while sklearn failed most of time.\n",
    "2. Both unigram and bigram were experimented in sklearn, didnâ€™t work well. NMF(Non-Negative Matrix) in sklearn, on other hand gave a better result, and it was much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897d14b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "history_visible": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
